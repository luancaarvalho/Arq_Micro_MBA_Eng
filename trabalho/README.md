# Pipeline CDC Fim a Fim

Este projeto implementa um pipeline completo de **Change Data Capture (CDC)** para replicaÃ§Ã£o de dados em tempo real, utilizando **mudanÃ§as lÃ³gicas do banco de dados** como fonte primÃ¡ria de dados. O pipeline demonstra a captura de mudanÃ§as (INSERT, UPDATE, DELETE) atravÃ©s da replicaÃ§Ã£o lÃ³gica do PostgreSQL e sua distribuiÃ§Ã£o para mÃºltiplos destinos.

## ğŸ¯ Conceito Principal

**A fonte de dados sÃ£o as mudanÃ§as lÃ³gicas capturadas diretamente do WAL (Write-Ahead Log) do PostgreSQL**, atravÃ©s da replicaÃ§Ã£o lÃ³gica (`wal_level=logical`). O Debezium conecta-se ao PostgreSQL via slot de replicaÃ§Ã£o lÃ³gica, capturando cada mudanÃ§a (INSERT, UPDATE, DELETE) em tempo real sem necessidade de modificar a aplicaÃ§Ã£o fonte ou usar polling.

## ğŸ“š InformaÃ§Ãµes do Projeto

**InstituiÃ§Ã£o**: Universidade de Fortaleza (UNIFOR)  
**Disciplina**: Arquitetura de MicroserviÃ§os  
**Curso**: MBA Engenharia de Dados

### ğŸ‘¥ Integrantes

- **Rafael Lima Tavares** - MatrÃ­cula: [ADICIONAR MATRÃCULA]
- **Dante Dantes** - MatrÃ­cula: [ADICIONAR MATRÃCULA]

## ğŸ¯ O que foi Desenvolvido

Este trabalho implementa um pipeline CDC completo e reprodutÃ­vel que demonstra:

1. **Captura de MudanÃ§as LÃ³gicas em Tempo Real**
   - **Fonte de dados: MudanÃ§as lÃ³gicas do banco** via replicaÃ§Ã£o lÃ³gica (WAL)
   - ConfiguraÃ§Ã£o do PostgreSQL com `wal_level=logical` para habilitar replicaÃ§Ã£o lÃ³gica
   - Uso do Debezium conectado via slot de replicaÃ§Ã£o lÃ³gica para capturar eventos de INSERT, UPDATE e DELETE
   - Captura em tempo real sem polling ou modificaÃ§Ãµes na aplicaÃ§Ã£o fonte
   - IntegraÃ§Ã£o com Kafka e Schema Registry usando Avro

2. **DistribuiÃ§Ã£o para MÃºltiplos Destinos**
   - **PostgreSQL Sink**: ReplicaÃ§Ã£o sÃ­ncrona usando JDBC Sink Connector
   - **MinIO (S3)**: Armazenamento em formato Parquet para anÃ¡lise de dados
   - Suporte para adicionar mais destinos (DuckDB, Elasticsearch, etc.)

3. **AutomaÃ§Ã£o e Reprodutibilidade**
   - Docker Compose para orquestraÃ§Ã£o de todos os serviÃ§os
   - Scripts automatizados para setup, carga inicial e validaÃ§Ã£o
   - DocumentaÃ§Ã£o completa com passo a passo

4. **ValidaÃ§Ã£o e Testes**
   - Scripts de teste para todas as operaÃ§Ãµes (INSERT/UPDATE/DELETE)
   - ValidaÃ§Ã£o automÃ¡tica da replicaÃ§Ã£o nos destinos
   - VerificaÃ§Ã£o de integridade dos dados

### ğŸ”§ Tecnologias Utilizadas

- **Apache Kafka**: Broker de mensagens para streaming de dados
- **Schema Registry**: Gerenciamento de schemas Avro
- **Debezium**: Conector source para CDC do PostgreSQL
- **Kafka Connect**: Framework para integraÃ§Ã£o de sistemas
- **PostgreSQL**: Banco de dados relacional (fonte e destino)
- **MinIO**: Armazenamento S3-compatible
- **Docker & Docker Compose**: ContainerizaÃ§Ã£o e orquestraÃ§Ã£o
- **Python**: Scripts de carga e testes
- **Avro**: SerializaÃ§Ã£o de dados com schema

## ğŸ“‹ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL Fonte   â”‚ (com replicaÃ§Ã£o lÃ³gica)
â”‚   (source_db)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ WAL (Write-Ahead Log)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Debezium Source   â”‚ (captura mudanÃ§as)
â”‚     Connector       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka + Schema    â”‚ (tÃ³picos Avro)
â”‚     Registry        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                 â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PostgreSQLâ”‚      â”‚  MinIO   â”‚      â”‚  DuckDB   â”‚
    â”‚  Sink    â”‚      â”‚  (S3)    â”‚      â”‚ (opcional)â”‚
    â”‚(sink_db) â”‚      â”‚ Parquet  â”‚      â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Componentes

- **Kafka**: Broker de mensagens
- **Schema Registry**: Gerenciamento de schemas Avro
- **Debezium**: Conector source para captura de mudanÃ§as do PostgreSQL
- **Kafka Connect**: Framework para conectar sistemas externos
- **PostgreSQL Fonte**: Banco de dados com replicaÃ§Ã£o lÃ³gica (WAL)
- **PostgreSQL Sink**: Banco de dados destino
- **MinIO**: Armazenamento S3-compatible para arquivos Parquet
- **DuckDB**: Banco de dados analÃ­tico local (opcional)

## ğŸ“ Estrutura do Projeto

```
trabalho/
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o de todos os serviÃ§os
â”œâ”€â”€ connectors/                 # ConfiguraÃ§Ãµes dos conectores
â”‚   â”œâ”€â”€ debezium-source.json    # Conector Debezium (source)
â”‚   â”œâ”€â”€ jdbc-sink-postgres.json # Conector JDBC Sink (PostgreSQL)
â”‚   â””â”€â”€ s3-sink-minio.json      # Conector S3 Sink (MinIO)
â”œâ”€â”€ scripts/                    # Scripts de execuÃ§Ã£o
â”‚   â”œâ”€â”€ setup.sh                # ConfiguraÃ§Ã£o inicial
â”‚   â”œâ”€â”€ initial_load.py         # Carga inicial de dados
â”‚   â”œâ”€â”€ mutations.py            # Testes de INSERT/UPDATE/DELETE
â”‚   â””â”€â”€ validate.sh             # ValidaÃ§Ã£o dos dados nos destinos
â”œâ”€â”€ docs/                       # DocumentaÃ§Ã£o adicional (opcional)
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â””â”€â”€ README.md                   # Este arquivo
```

## ğŸš€ Como Executar

### PrÃ©-requisitos

- **Docker** e **Docker Compose** instalados
- **Python 3.8+** com `pip`
- **Git** (para clonar o repositÃ³rio)

### Passo 1: Instalar DependÃªncias Python

```bash
cd trabalho
pip install -r requirements.txt
```

### Passo 2: Subir os ServiÃ§os

```bash
docker-compose up -d
```

Este comando irÃ¡ iniciar:
- Zookeeper
- Kafka
- Schema Registry
- PostgreSQL Fonte (porta 5433)
- PostgreSQL Sink (porta 5434)
- MinIO (portas 9000 e 9001)
- Kafka Connect (porta 8083)

**â±ï¸ Aguarde aproximadamente 60-90 segundos** para todos os serviÃ§os estarem prontos.

### Passo 3: Configurar o Pipeline

Execute o script de setup que irÃ¡:
- Criar a tabela `products` no PostgreSQL fonte
- Configurar o bucket no MinIO
- Registrar os conectores (Debezium, JDBC Sink, S3 Sink)

```bash
bash scripts/setup.sh
```

**Importante**: O script aguarda automaticamente os serviÃ§os estarem prontos antes de continuar.

### Passo 4: Carga Inicial

Insere dados iniciais na tabela `products` do banco fonte:

```bash
python scripts/initial_load.py
```

Este script insere 5 produtos iniciais que serÃ£o capturados pelo Debezium e replicados para os destinos.

### Passo 5: Testar MutaÃ§Ãµes (INSERT/UPDATE/DELETE)

Execute o script que testa todas as operaÃ§Ãµes:

```bash
python scripts/mutations.py
```

Este script executa:
1. **INSERT**: InserÃ§Ã£o de um novo produto
2. **UPDATE**: AtualizaÃ§Ã£o de preÃ§o e estoque
3. **DELETE**: RemoÃ§Ã£o de um produto
4. **MÃšLTIPLAS ATUALIZAÃ‡Ã•ES**: VÃ¡rias atualizaÃ§Ãµes sequenciais

### Passo 6: Validar os Dados

Verifique se os dados foram replicados corretamente nos destinos:

```bash
bash scripts/validate.sh
```

Este script mostra:
- Total de produtos no PostgreSQL Sink
- Lista de produtos replicados
- ComparaÃ§Ã£o entre fonte e sink
- Arquivos no MinIO (S3)
- Status dos conectores
- InformaÃ§Ãµes sobre os tÃ³picos Kafka

## ğŸ” VerificaÃ§Ãµes Manuais

### Verificar Dados no PostgreSQL Sink

```bash
docker exec -it postgres-sink psql -U postgres -d sink_db -c "SELECT * FROM products ORDER BY id;"
```

### Verificar TÃ³picos Kafka

```bash
docker exec kafka kafka-topics --list --bootstrap-server kafka:29092
```

### Verificar Mensagens no TÃ³pico

```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server kafka:29092 \
  --topic cdc-source-server.public.products \
  --from-beginning \
  --max-messages 5
```

### Verificar Arquivos no MinIO

Acesse o console do MinIO em: http://localhost:9001
- UsuÃ¡rio: `minioadmin`
- Senha: `minioadmin`

Ou via CLI:

```bash
docker exec minio mc ls local/cdc-data --recursive
```

### Verificar Status dos Conectores

```bash
# Listar todos os conectores
curl http://localhost:8083/connectors

# Status de um conector especÃ­fico
curl http://localhost:8083/connectors/debezium-postgres-source/status | python3 -m json.tool
```

## ğŸ“Š Interfaces Web

- **Kafka UI**: http://localhost:8080 (se configurado)
- **MinIO Console**: http://localhost:9001
  - UsuÃ¡rio: `minioadmin`
  - Senha: `minioadmin`
- **Schema Registry**: http://localhost:8082

## ğŸ› Troubleshooting

### Problema: Conectores nÃ£o iniciam

**SoluÃ§Ã£o**: Verifique os logs do Kafka Connect:

```bash
docker logs kafka-connect
```

### Problema: Debezium nÃ£o captura mudanÃ§as

**VerificaÃ§Ãµes**:
1. Confirme que o PostgreSQL fonte tem `wal_level=logical`:
   ```bash
   docker exec postgres-source psql -U postgres -d source_db -c "SHOW wal_level;"
   ```

2. Verifique se o slot de replicaÃ§Ã£o foi criado:
   ```bash
   docker exec postgres-source psql -U postgres -d source_db -c "SELECT * FROM pg_replication_slots;"
   ```

3. Verifique os logs do Debezium:
   ```bash
   docker logs kafka-connect | grep debezium
   ```

### Problema: Dados nÃ£o aparecem no Sink

**VerificaÃ§Ãµes**:
1. Confirme que o conector JDBC Sink estÃ¡ RUNNING:
   ```bash
   curl http://localhost:8083/connectors/jdbc-sink-postgres/status
   ```

2. Verifique se hÃ¡ erros no conector:
   ```bash
   docker logs kafka-connect | grep -i error
   ```

3. Confirme que o tÃ³pico tem mensagens:
   ```bash
   docker exec kafka kafka-console-consumer \
     --bootstrap-server kafka:29092 \
     --topic cdc-source-server.public.products \
     --from-beginning \
     --max-messages 1
   ```

### Problema: Arquivos nÃ£o aparecem no MinIO

**Causa**: O S3 Sink Connector sÃ³ escreve arquivos quando:
- O `flush.size` Ã© atingido (10 mensagens por padrÃ£o neste projeto)
- O `rotate.interval.ms` Ã© atingido (30 segundos por padrÃ£o neste projeto)

**SoluÃ§Ã£o**: 
1. Aguarde alguns segundos apÃ³s executar mutaÃ§Ãµes
2. Execute mais mutaÃ§Ãµes para atingir o `flush.size`
3. Verifique os logs do S3 connector:
   ```bash
   docker logs kafka-connect | grep s3-sink
   ```

**Nota**: O S3 connector estÃ¡ configurado com credenciais fixas para MinIO (`minioadmin/minioadmin`). Se vocÃª alterar as credenciais do MinIO, atualize tambÃ©m o arquivo `connectors/s3-sink-minio.json`.

### Problema: Erro de conexÃ£o com MinIO

**SoluÃ§Ã£o**: Verifique se o MinIO estÃ¡ rodando:

```bash
docker ps | grep minio
```

Se nÃ£o estiver, reinicie:

```bash
docker-compose restart minio
```

## ğŸ§¹ Limpeza

Para parar e remover todos os containers e volumes:

```bash
docker-compose down -v
```

**âš ï¸ AtenÃ§Ã£o**: Isso apagarÃ¡ todos os dados!

Para apenas parar os serviÃ§os (mantendo dados):

```bash
docker-compose stop
```

## ğŸ“ EvidÃªncias de ExecuÃ§Ã£o

### Consultas de ValidaÃ§Ã£o

#### PostgreSQL Sink - Contar produtos
```sql
SELECT COUNT(*) FROM products;
```

#### PostgreSQL Sink - Listar todos os produtos
```sql
SELECT id, name, price, stock, category, updated_at 
FROM products 
ORDER BY id;
```

#### PostgreSQL Sink - Verificar Ãºltima atualizaÃ§Ã£o
```sql
SELECT MAX(updated_at) as ultima_atualizacao FROM products;
```

#### Comparar Fonte vs Sink
```bash
# Fonte
docker exec postgres-source psql -U postgres -d source_db -c "SELECT COUNT(*) FROM products;"

# Sink
docker exec postgres-sink psql -U postgres -d sink_db -c "SELECT COUNT(*) FROM products;"
```

### Verificar Schemas no Schema Registry

```bash
curl http://localhost:8082/subjects | python3 -m json.tool
```

## ğŸ¯ Checklist de Entrega

- [x] `docker-compose.yml` com todos os serviÃ§os
- [x] Conectores configurados (Debezium source + 2+ sinks)
- [x] Scripts de carga inicial e mutaÃ§Ãµes
- [x] Scripts de validaÃ§Ã£o
- [x] README com instruÃ§Ãµes passo a passo
- [x] DemonstraÃ§Ã£o de INSERT/UPDATE/DELETE
- [x] EvidÃªncias de replicaÃ§Ã£o para â‰¥ 2 destinos

## ğŸ“š ReferÃªncias

- [Debezium Documentation](https://debezium.io/documentation/)
- [Kafka Connect Documentation](https://kafka.apache.org/documentation/#connect)
- [Confluent S3 Sink Connector](https://docs.confluent.io/kafka-connect-s3-sink/current/overview.html)
- [PostgreSQL Logical Replication](https://www.postgresql.org/docs/current/logical-replication.html)

## ğŸ“ ConclusÃ£o

Este trabalho demonstra a implementaÃ§Ã£o prÃ¡tica de um pipeline CDC completo, desde a captura de mudanÃ§as no banco de dados fonte atÃ© a distribuiÃ§Ã£o para mÃºltiplos destinos, utilizando tecnologias modernas de streaming de dados e integraÃ§Ã£o.

### âœ… Objetivos AlcanÃ§ados

- âœ… Pipeline CDC reprodutÃ­vel com Docker Compose
- âœ… Captura de INSERT, UPDATE e DELETE em tempo real
- âœ… ReplicaÃ§Ã£o para pelo menos 2 destinos (PostgreSQL e MinIO)
- âœ… Uso de Schema Registry com Avro
- âœ… Scripts automatizados de setup, carga e validaÃ§Ã£o
- âœ… DocumentaÃ§Ã£o completa e passo a passo

### ğŸ“Š EvidÃªncias de ExecuÃ§Ã£o

O pipeline foi testado e validado com sucesso, demonstrando:
- Captura de mudanÃ§as via Debezium
- ReplicaÃ§Ã£o sÃ­ncrona para PostgreSQL Sink
- Armazenamento em formato Parquet no MinIO
- Integridade dos dados entre fonte e destinos

---

## ğŸ‘¥ Autores

**Rafael Lima Tavares** - MatrÃ­cula: [ADICIONAR MATRÃCULA]  
**Dante Dantes** - MatrÃ­cula: [ADICIONAR MATRÃCULA]

**InstituiÃ§Ã£o**: Universidade de Fortaleza (UNIFOR)  
**Disciplina**: Arquitetura de MicroserviÃ§os  
**Curso**: MBA Engenharia de Dados

---

**Ãšltima atualizaÃ§Ã£o**: Dezembro 2024

